{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marimarmatos/AnyoneAI/blob/main/21_1_PRACTICE_Introduction_to_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdzpYbE7ZIjt"
      },
      "source": [
        "# Prompt Engineering Practice\n",
        "\n",
        "This notebook provides a hands-on opportunity to practice prompt engineering, a crucial skill for effectively interacting with large language models (LLMs). Crafting clear, concise, and well-structured prompts is essential for obtaining desired outputs and unlocking the full potential of LLMs.\n",
        "\n",
        "Mastering prompt engineering is fundamental for more advanced LLM applications and training methodologies, such as Reinforcement Learning from Human Feedback (RLHF). By understanding how to guide LLMs through prompt design, you lay the groundwork for fine-tuning models and aligning their behavior with specific goals and desired outcomes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's start by setting the API key\n",
        "import json\n",
        "import openai\n",
        "\n",
        "# Read the API key from a JSON file\n",
        "with open('api_gpt.json', 'r') as file:\n",
        "    config = json.load(file)\n",
        "\n",
        "openai.api_key = config['api_key']"
      ],
      "metadata": {
        "id": "S3IU5nbgzt3m"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Task\n",
        "Generate a simple system prompt that takes the name of an algorithm, and asks for a response that should only contain code with several test case."
      ],
      "metadata": {
        "id": "B6Doa8-V4iJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put an algorithm name here\n",
        "algorithm_name = \"count of dogs in DataFrames\"\n",
        "# Create the system prompt\n",
        "system_prompt = f\"\"\"\n",
        "You are an expert in programming.\n",
        "Given the name of an algorithm, you will respond solely with the implementation code of that algorithm,\n",
        "including several test cases to demonstrate its functionality.\n",
        "Do not include any explanations or additional text.\n",
        "\n",
        "Algorithm Name:\n",
        "{algorithm_name}\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "LBotVUK847fs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the model\n",
        "completion = openai.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": system_prompt.format(algorithm_name)}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "metadata": {
        "id": "fympGswk5Ppz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe1dbee9-bd0c-4bc3-b901-cc4a9588a3d5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "def count_dogs(df: pd.DataFrame) -> int:\n",
            "    return df['animal'].str.lower().str.count('dog').sum()\n",
            "\n",
            "# Test cases\n",
            "data1 = {'animal': ['Dog', 'Cat', 'fish', 'dog', 'dog', 'Mouse']}\n",
            "df1 = pd.DataFrame(data1)\n",
            "print(count_dogs(df1))  # Output: 3\n",
            "\n",
            "data2 = {'animal': ['dog', 'dog', 'Dog', 'DOG', 'cat']}\n",
            "df2 = pd.DataFrame(data2)\n",
            "print(count_dogs(df2))  # Output: 4\n",
            "\n",
            "data3 = {'animal': ['Cat', 'Fish', 'Hamster']}\n",
            "df3 = pd.DataFrame(data3)\n",
            "print(count_dogs(df3))  # Output: 0\n",
            "\n",
            "data4 = {'animal': ['Dog', 'Dog', 'cat', 'dog', 'Tiger', 'DOG']}\n",
            "df4 = pd.DataFrame(data4)\n",
            "print(count_dogs(df4))  # Output: 4\n",
            "\n",
            "data5 = {'animal': []}\n",
            "df5 = pd.DataFrame(data5)\n",
            "print(count_dogs(df5))  # Output: 0\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Task\n",
        "Situation: You're curating a dataset containing prompts that ask for Image Generation tasks. However, the dataset contains prompts that are not usable for the project that you're working on right now. Currently, you want only non ambigous and human-like prompts. To help you classify the prompts, you want to create a system template that gives you a score between 0 and 1 (with 0 being an unuseful prompt and 1 a perfect prompt) and a brieft explanation.\n",
        "\n",
        "### Definitions:\n",
        "Unambigous: The prompts needs to explicitly mention figures and some visual details.\n",
        "\n",
        "Humanlike: The prompt should sound human, containing a some personality, not just a very robotic list of requirements"
      ],
      "metadata": {
        "id": "bL9JEAA_nUz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here are some examples:\n",
        "# Ambigous prompts\n",
        "ambiguous_prompt_1 = \"Create an image of a city.\"\n",
        "ambiguous_prompt_2 = \"Generate an artwork of a park setting.\"\n",
        "\n",
        "# Non-Humanlike Prompts\n",
        "non_humanlike_prompt_1 = \"Illustrate a scene: Elephant under tree, sunny backdrop, time: midday, color: realistic.\"\n",
        "non_humanlike_prompt_2 = \"Render image: Two children playing with ball in park, include details: bench, kite in sky, day background.\"\n",
        "\n",
        "# Perfect prompts\n",
        "perfect_prompt_1 = \"Imagine a cozy reading nook with a plush armchair, a small wooden side table, and a window with morning sunlight filtering in through soft, sheer curtains. Could you illustrate this warm, inviting scene?\"\n",
        "perfect_prompt_2 = \"I'd love to see a serene beach at sunset, with gentle waves lapping at the shore and a lone seagull gliding across a vibrant orange and pink sky. Can you create this tranquil moment?\"\n",
        "\n",
        "# Task: Create yourself two prompts that would be perfect\n",
        "your_prompt_1 = \"create a sunset at the beach with orange and pink colors, two mountains on either side of the horizon and a fishing boat on the left side of the image\"\n",
        "your_prompt_2 = \"create an orange kitten using a blue toy cap\""
      ],
      "metadata": {
        "id": "BLjiMYc4nUMz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are an expert evaluator in classifying prompts for image generation, responsible for determining how useful they are for a specific project. For each prompt provided, you must do the following:\n",
        "\n",
        "Rating: Assign a score of 0 or 1, not decimals, only scores 0 or 1, where:\n",
        "\n",
        "0 indicates that the prompt is not useful, ambiguous, or irrelevant for image generation.\n",
        "1 indicates that the prompt is excellent, clear, specific, and appropriate for generating high-quality images.\n",
        "Explanation: Provide a brief justification in 2 or 3 sentences explaining why you assigned that particular score, highlighting positive or negative aspects based on the following criteria:\n",
        "\n",
        "Unambiguity: Does the prompt explicitly mention figures, objects, or visual details? Useful prompts should clearly describe what needs to be generated with concrete details.\n",
        "Humanlike/Personality: Does the prompt sound natural and human? It should have a conversational tone, include nuances or characteristics that reflect human intent or style rather than mechanical or cold lists.\n",
        "Clarity and Precision: Is the prompt clear, unambiguous, and with well-defined visual expectations? Precise prompts help achieve more suitable and coherent results.\n",
        "Response: Only provide the score (a decimal between 0 and 1) and the brief explanation, without additional comments or debates.\n",
        "\n",
        "Here goes the prompt to evaluate:\n",
        "{}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MUIrD8H9zEQV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(system_prompt.format(ambiguous_prompt_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvvS-YVs0y8B",
        "outputId": "a748cd2d-c130-4f98-fba6-c31dd405d50f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an expert evaluator in classifying prompts for image generation, responsible for determining how useful they are for a specific project. For each prompt provided, you must do the following:\n",
            "\n",
            "Rating: Assign a score of 0 or 1, where:\n",
            "\n",
            "0 indicates that the prompt is not useful, ambiguous, or irrelevant for image generation.\n",
            "1 indicates that the prompt is excellent, clear, specific, and appropriate for generating high-quality images.\n",
            "Explanation: Provide a brief justification in 2 or 3 sentences explaining why you assigned that particular score, highlighting positive or negative aspects based on the following criteria:\n",
            "\n",
            "Unambiguity: Does the prompt explicitly mention figures, objects, or visual details? Useful prompts should clearly describe what needs to be generated with concrete details.\n",
            "Humanlike/Personality: Does the prompt sound natural and human? It should have a conversational tone, include nuances or characteristics that reflect human intent or style rather than mechanical or cold lists.\n",
            "Clarity and Precision: Is the prompt clear, unambiguous, and with well-defined visual expectations? Precise prompts help achieve more suitable and coherent results.\n",
            "Response: Only provide the score (a decimal between 0 and 1) and the brief explanation, without additional comments or debates.\n",
            "\n",
            "Here goes the prompt to evaluate:\n",
            "Create an image of a city.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an autograder function\n",
        "def call_autograder(prompt):\n",
        "  completion = openai.chat.completions.create(\n",
        "  model=\"gpt-4o-mini\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "\n",
        "    ]\n",
        "  )\n",
        "  return completion.choices[0].message.content\n",
        "\n",
        "# Call the model with each prompt to evaluate the results\n",
        "prompt_list = [ambiguous_prompt_1,\n",
        "              ambiguous_prompt_2,\n",
        "              non_humanlike_prompt_1,\n",
        "              non_humanlike_prompt_2,\n",
        "              perfect_prompt_1,\n",
        "              perfect_prompt_2,\n",
        "              your_prompt_1,\n",
        "              your_prompt_2]\n",
        "\n",
        "for prompt_to_evaluate in prompt_list:\n",
        "  full_prompt = system_prompt.format(prompt_to_evaluate)\n",
        "  result = call_autograder(full_prompt)\n",
        "  print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjsrh42hzKZS",
        "outputId": "f1964547-c1cd-4613-e69d-ba7a9acc39b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: The prompt is too vague and lacks specific details about the city that should be depicted, such as architectural style, time of day, or any distinctive features. Without clearer visual expectations or descriptions, it makes it difficult to generate a meaningful and coherent image.\n",
            "0  \n",
            "The prompt is too vague and lacks specific details about the park setting, such as the type of park, time of day, season, or any particular elements to include. This ambiguity may lead to varied interpretations, resulting in an unclear or unsatisfactory image.\n",
            "1: The prompt is clear and specific, explicitly detailing the elements to include in the image, such as the elephant, tree, and sunny backdrop. It provides a well-defined visual expectation with a realistic color scheme, making it useful for generating a high-quality image.\n",
            "1  \n",
            "The prompt is clear and specific, providing explicit details about the scene including the figures (two children), objects (ball, bench, kite), and setting (park, day background). This specificity helps in generating a coherent and high-quality image that visually represents the intended concept.\n",
            "1  \n",
            "This prompt is clear and specific about the elements that need to be included in the image, such as a plush armchair, wooden side table, and particular lighting conditions. It uses a conversational tone and presents a vivid scene, making it conducive for generating a high-quality, inviting image.\n",
            "1  \n",
            "The prompt is clear and specific, detailing the scene with vivid imagery such as \"serene beach,\" \"gentle waves,\" and \"lone seagull.\" This explicit mention of visual elements ensures that the image generation is focused and coherent, creating a tranquil atmosphere that reflects the human intent for a beautiful sunset moment.\n",
            "1  \n",
            "This prompt is excellent as it clearly describes a specific scene with distinct visual elements: a sunset at the beach, specific colors, mountains, and a fishing boat. It is unambiguous and provides precise details that will help generate a high-quality image. The tone is conversational and natural, reflecting human intent in the description.\n",
            "1  \n",
            "The prompt is clear and specific, detailing both the subject (an orange kitten) and an accompanying object (a blue toy cap), which provides a solid foundation for image generation. It sounds natural and is likely to yield a coherent and visually interesting image.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Gemini API"
      ],
      "metadata": {
        "id": "9C7TZS6e_ie-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the Python SDK\n",
        "import google.generativeai as genai\n",
        "import json\n",
        "\n",
        "# Read the API key from a JSON file\n",
        "with open('api_google.json', 'r') as file:\n",
        "    config = json.load(file)\n",
        "\n",
        "\n",
        "\n",
        "genai.configure(api_key=config['api_key'])\n",
        "\n",
        "# Initialize the Gemini API\n",
        "gemini_model = genai.GenerativeModel('gemini-1.5-flash-latest')"
      ],
      "metadata": {
        "id": "r_TB8292_lkg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "qKnS1WwP_22t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Put an algorithm name here\n",
        "algorithm_name = \"count of dogs in DataFrames\"\n",
        "# Create the system prompt\n",
        "system_prompt = f\"\"\"\n",
        "You are an expert in programming.\n",
        "Given the name of an algorithm, you will respond solely with the implementation code of that algorithm,\n",
        "including several test cases to demonstrate its functionality.\n",
        "Do not include any explanations or additional text.\n",
        "\n",
        "Algorithm Name:\n",
        "{algorithm_name}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "nN2dRxvQ_4qz"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the model with the new prompt\n",
        "response = gemini_model.generate_content(system_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "yBaeG9UgARFQ",
        "outputId": "357eba14-378d-4e76-ea66-0ade3227730d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import pandas as pd\n",
            "\n",
            "def count_dogs(df, dog_column):\n",
            "    try:\n",
            "        return df[dog_column].value_counts().get('dog', 0)\n",
            "    except KeyError:\n",
            "        return 0\n",
            "\n",
            "\n",
            "#Test Cases\n",
            "data1 = {'animal': ['dog', 'cat', 'dog', 'bird']}\n",
            "df1 = pd.DataFrame(data1)\n",
            "print(count_dogs(df1, 'animal')) #Output: 2\n",
            "\n",
            "data2 = {'pet': ['cat', 'cat', 'bird']}\n",
            "df2 = pd.DataFrame(data2)\n",
            "print(count_dogs(df2, 'pet')) # Output: 0\n",
            "\n",
            "data3 = {'animal': ['dog', 'dog', 'dog']}\n",
            "df3 = pd.DataFrame(data3)\n",
            "print(count_dogs(df3, 'animal')) #Output:3\n",
            "\n",
            "data4 = {'animal': [1,2,3]}\n",
            "df4 = pd.DataFrame(data4)\n",
            "print(count_dogs(df4, 'animal')) # Output: 0\n",
            "\n",
            "data5 = {}\n",
            "df5 = pd.DataFrame(data5)\n",
            "print(count_dogs(df5, 'animal')) # Output: 0\n",
            "\n",
            "data6 = {'animal': ['dog', 'cat', 'Dog', 'DOG']}\n",
            "df6 = pd.DataFrame(data6)\n",
            "print(count_dogs(df6, 'animal')) # Output: 1\n",
            "\n",
            "data7 = {'animal': ['dog', 'cat', 'dog', 'bird', None]}\n",
            "df7 = pd.DataFrame(data7)\n",
            "print(count_dogs(df7, 'animal')) # Output: 2\n",
            "\n",
            "\n",
            "```\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "kwnZpDRsBJsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtA3ikURBLME"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}